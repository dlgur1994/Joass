{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_main_model.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vr8NyThXmEb8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":131},"outputId":"9e5707f8-e519-499e-968f-74ebdfbd413e","executionInfo":{"status":"ok","timestamp":1558077723826,"user_tz":-540,"elapsed":30507,"user":{"displayName":"전재민학부생","photoUrl":"","userId":"13182960211319440802"}}},"source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p2gAFUM-mUvK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3472},"outputId":"c2c3661c-8597-49eb-d56b-1a93c8012804"},"source":["#새로운 model을 만드는 NN, test accuracy는 생략 - 전체 feature에 대한 training accuracy만 측정\n","#_*_coding: utf-8_*_\n","\n","import tensorflow as tf\n","import numpy as np\n","\n","#path to the directory\n","dirname = '/gdrive/My Drive/18-winter/정리판/main/'\n","save_path = '/gdrive/My Drive/19-1/model/main/'\n","data = np.load(dirname + 'data/data.npz')\n","x_data = data['X']\n","y_data = data['Y']\n","\n","learning_rate = 0.1\n","training_epochs = 10000000\n","\n","n_dim = 39\n","n_classes = 9\n","\n","n_hid1 = 300\n","n_hid2 = 200\n","n_hid3 = 100\n","\n","X = tf.placeholder(tf.float32, [None, n_dim], name = 'X')\n","Y = tf.placeholder(tf.float32, [None, n_classes], name = 'Y')\n","\n","stddev = 0.1 #standard deviation??\n","\n","#WEIGHTS\n","W1 = tf.Variable(tf.random_normal([n_dim, n_hid1], stddev = stddev), name = \"w1\")\n","W2 = tf.Variable(tf.random_normal([n_hid1, n_hid2], stddev = stddev), name = \"w2\")\n","W3 = tf.Variable(tf.random_normal([n_hid2, n_hid3], stddev = stddev), name = \"w3\")\n","W = tf.Variable(tf.random_normal([n_hid3, n_classes], stddev = stddev), name = \"w\")\n","\n","#BIASES\n","b1 = tf.Variable(tf.random_normal([n_hid1]), name = \"b1\")\n","b2 = tf.Variable(tf.random_normal([n_hid2]), name = \"b2\")\n","b3 = tf.Variable(tf.random_normal([n_hid3]), name = \"b3\")\n","b = tf.Variable(tf.random_normal([n_classes]), name = \"b\")\n","\n","#laters\n","layer1 = tf.nn.sigmoid(tf.matmul(X, W1) + b1, name = \"layer1\")\n","layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2, name = \"layer2\")\n","layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3, name = \"layer3\")\n","\n","#dropout\n","keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")\n","dropout = tf.nn.dropout(layer3, keep_prob, name = \"dropout\")\n","\n","#y_hat\n","y = tf.nn.softmax(tf.matmul(dropout, W) + b)\n","\n","cross_entropy = tf.reduce_mean(-Y*tf.log(tf.clip_by_value(y, 1e-10, 1.0)), name = \"cost\")\n","\n","train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n","\n","#accuracy\n","correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(Y, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"accuracy\")\n","\n","init = tf.global_variables_initializer()\n","\n","#saver\n","saver = tf.train.Saver()\n","\n","epoch = 0\n","train_accuracy = 0\n","\n","with tf.Session() as sess:\n","    sess.run(init)\n","\n","    for epoch in range(training_epochs):\n","        sess.run(train_step, feed_dict = {X: x_data, Y: y_data, keep_prob: 0.5})\n","\n","        #epoch 1000마다 저장\n","        if epoch%1000 == 0:\n","            train_accuracy, hypo = sess.run([accuracy, y], feed_dict = {X: x_data, Y: y_data, keep_prob: 1.0})\n","            print('step: ',epoch,', training accuracy: ',train_accuracy)\n","           \n","            saver.save(sess, save_path + 'main_model.ckpt')\n","            print(\"save %d\"%epoch)\n","\n","        #accuracy 90% 이상이면 학습 종료\n","        if 0.9 < train_accuracy:\n","          break\n","        if hypo[0][0] == 'nan':\n","          break\n","\n","    print(sess.run([accuracy, y], feed_dict = {X: x_data, Y: y_data, keep_prob: 1.0}))\n","    saver.save(sess, save_path + 'main_model.ckpt')\n","\n","    data.close()\n","    print(\"save the model\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-2-254f74dec3eb>:46: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","step:  0 , training accuracy:  0.122744575\n","save 0\n","step:  1000 , training accuracy:  0.38404873\n","save 1000\n","step:  2000 , training accuracy:  0.4281576\n","save 2000\n","step:  3000 , training accuracy:  0.45426127\n","save 3000\n","step:  4000 , training accuracy:  0.48723465\n","save 4000\n","step:  5000 , training accuracy:  0.49904498\n","save 5000\n","step:  6000 , training accuracy:  0.5123706\n","save 6000\n","step:  7000 , training accuracy:  0.5264029\n","save 7000\n","step:  8000 , training accuracy:  0.5358894\n","save 8000\n","step:  9000 , training accuracy:  0.53590846\n","save 9000\n","step:  10000 , training accuracy:  0.54659826\n","save 10000\n","step:  11000 , training accuracy:  0.5488139\n","save 11000\n","step:  12000 , training accuracy:  0.5569888\n","save 12000\n","step:  13000 , training accuracy:  0.5583067\n","save 13000\n","step:  14000 , training accuracy:  0.5675958\n","save 14000\n","step:  15000 , training accuracy:  0.5701616\n","save 15000\n","step:  16000 , training accuracy:  0.5734532\n","save 16000\n","step:  17000 , training accuracy:  0.5767894\n","save 17000\n","step:  18000 , training accuracy:  0.5739498\n","save 18000\n","step:  19000 , training accuracy:  0.5817873\n","save 19000\n","step:  20000 , training accuracy:  0.5808768\n","save 20000\n","step:  21000 , training accuracy:  0.5813989\n","save 21000\n","step:  22000 , training accuracy:  0.58363366\n","save 22000\n","step:  23000 , training accuracy:  0.5873454\n","save 23000\n","step:  24000 , training accuracy:  0.5875683\n","save 24000\n","step:  25000 , training accuracy:  0.5919486\n","save 25000\n","step:  26000 , training accuracy:  0.59078985\n","save 26000\n","step:  27000 , training accuracy:  0.5938268\n","save 27000\n","step:  28000 , training accuracy:  0.5967237\n","save 28000\n","step:  29000 , training accuracy:  0.5949601\n","save 29000\n","step:  30000 , training accuracy:  0.59557766\n","save 30000\n","step:  31000 , training accuracy:  0.59909844\n","save 31000\n","step:  32000 , training accuracy:  0.5986146\n","save 32000\n","step:  33000 , training accuracy:  0.60049915\n","save 33000\n","step:  34000 , training accuracy:  0.60246646\n","save 34000\n","step:  35000 , training accuracy:  0.6017534\n","save 35000\n","step:  36000 , training accuracy:  0.6018425\n","save 36000\n","step:  37000 , training accuracy:  0.6052615\n","save 37000\n","step:  38000 , training accuracy:  0.6040391\n","save 38000\n","step:  39000 , training accuracy:  0.60396904\n","save 39000\n","step:  40000 , training accuracy:  0.6016388\n","save 40000\n","step:  41000 , training accuracy:  0.6062165\n","save 41000\n","step:  42000 , training accuracy:  0.6101512\n","save 42000\n","step:  43000 , training accuracy:  0.6105459\n","save 43000\n","step:  44000 , training accuracy:  0.610495\n","save 44000\n","step:  45000 , training accuracy:  0.60996014\n","save 45000\n","step:  46000 , training accuracy:  0.60764265\n","save 46000\n","step:  47000 , training accuracy:  0.6131881\n","save 47000\n","step:  48000 , training accuracy:  0.60938716\n","save 48000\n","step:  49000 , training accuracy:  0.6148116\n","save 49000\n","step:  50000 , training accuracy:  0.61509174\n","save 50000\n","step:  51000 , training accuracy:  0.61025935\n","save 51000\n","step:  52000 , training accuracy:  0.6154101\n","save 52000\n","step:  53000 , training accuracy:  0.6171164\n","save 53000\n","step:  54000 , training accuracy:  0.6163396\n","save 54000\n","step:  55000 , training accuracy:  0.6182051\n","save 55000\n","step:  56000 , training accuracy:  0.6152318\n","save 56000\n","step:  57000 , training accuracy:  0.615461\n","save 57000\n","step:  58000 , training accuracy:  0.61449325\n","save 58000\n","step:  59000 , training accuracy:  0.61482435\n","save 59000\n","step:  60000 , training accuracy:  0.61661977\n","save 60000\n","step:  61000 , training accuracy:  0.61679167\n","save 61000\n","step:  62000 , training accuracy:  0.6195676\n","save 62000\n","step:  63000 , training accuracy:  0.61982226\n","save 63000\n","step:  64000 , training accuracy:  0.61924285\n","save 64000\n","step:  65000 , training accuracy:  0.6222416\n","save 65000\n","step:  66000 , training accuracy:  0.62119746\n","save 66000\n","step:  67000 , training accuracy:  0.6220379\n","save 67000\n","step:  68000 , training accuracy:  0.62410706\n","save 68000\n","step:  69000 , training accuracy:  0.62492836\n","save 69000\n","step:  70000 , training accuracy:  0.620287\n","save 70000\n","step:  71000 , training accuracy:  0.62621444\n","save 71000\n","step:  72000 , training accuracy:  0.6231457\n","save 72000\n","step:  73000 , training accuracy:  0.62418985\n","save 73000\n","step:  74000 , training accuracy:  0.6231075\n","save 74000\n","step:  75000 , training accuracy:  0.62629086\n","save 75000\n","step:  76000 , training accuracy:  0.6257115\n","save 76000\n","step:  77000 , training accuracy:  0.6194848\n","save 77000\n","step:  78000 , training accuracy:  0.62620175\n","save 78000\n","step:  79000 , training accuracy:  0.6211147\n","save 79000\n","step:  80000 , training accuracy:  0.6263036\n","save 80000\n","step:  81000 , training accuracy:  0.6279017\n","save 81000\n","step:  82000 , training accuracy:  0.62727135\n","save 82000\n","step:  83000 , training accuracy:  0.62646276\n","save 83000\n","step:  84000 , training accuracy:  0.6266538\n","save 84000\n","step:  85000 , training accuracy:  0.6296016\n","save 85000\n","step:  86000 , training accuracy:  0.6280736\n","save 86000\n","step:  87000 , training accuracy:  0.6293978\n","save 87000\n","step:  88000 , training accuracy:  0.6310277\n","save 88000\n"],"name":"stdout"}]}]}